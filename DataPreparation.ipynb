{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24fa1089",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce35b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from importlib.metadata import version\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e226d",
   "metadata": {},
   "source": [
    "# Tokenization with tiktoken\n",
    "\n",
    "This cell demonstrates how to use the `tiktoken` library for tokenizing and detokenizing text, which is essential for working with language models such as those from OpenAI.\n",
    "\n",
    "## Steps Covered\n",
    "1. **Install and Import tiktoken**: Ensure the `tiktoken` library is installed and import it along with the `version` utility.\n",
    "2. **Check tiktoken Version**: Print the installed version of `tiktoken` to verify the setup.\n",
    "3. **Initialize Tokenizer**: Load the `cl100k_base` encoding, which is commonly used for OpenAI models.\n",
    "4. **Explore Vocabulary Size**: Display the size of the tokenizer's vocabulary.\n",
    "5. **Tokenize Sample Text**: Encode a sample string into tokens and display the result.\n",
    "6. **Decode Tokens**: Convert the tokens back to text and verify the output.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** Tokenization is the process of converting text into a sequence of tokens (numbers) that a model can understand. Detokenization is the reverse process, converting tokens back into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5f41ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version: 0.11.0\n",
      "Vocabulary size: 100277\n",
      "Encoded tokens: [9906, 11, 1917, 0, 1115, 374, 264, 1296, 315, 1268, 1664, 279, 47058, 4375, 13]\n",
      "Decoded text: Hello, world! This is a test of how well the tokenizer works.\n"
     ]
    }
   ],
   "source": [
    "print(f'Tiktoken version: {version('tiktoken')}')\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(f'Vocabulary size: {tokenizer.n_vocab}')\n",
    "\n",
    "sample_text = \"Hello, world! This is a test of how well the tokenizer works.\"\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(f'Encoded tokens: {tokens}')\n",
    "\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f'Decoded text: {decoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae8fffc",
   "metadata": {},
   "source": [
    "# Combine Text Files for LLM Training\n",
    "\n",
    "This cell merges all `.txt` files from the `books` directory into a single file, `all_books.txt`, with each book separated by an `<EOS>` (End Of Sequence) token. This is a common preprocessing step for language model training, allowing the model to learn document boundaries.\n",
    "\n",
    "## What the Code Does\n",
    "- **Finds all `.txt` files** in the `books` directory using `Path.glob`.\n",
    "- **Opens `all_books.txt`** for writing in UTF-8 encoding.\n",
    "- **Iterates through each file**, reads its content, and writes it to the output file.\n",
    "- **Appends `<EOS>`** after each book to mark the end of a document.\n",
    "\n",
    "---\n",
    "\n",
    "> **Why use `<EOS>`?**\n",
    ">\n",
    "> The `<EOS>` token helps the language model distinguish where one document ends and another begins. This is important for tasks like text generation, where you want the model to respect document boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a3564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path('./books').glob('*.txt')\n",
    "with open('all_books.txt', 'w', encoding='utf-8') as outfile:\n",
    "    for file in files:\n",
    "        outfile.write(Path(file).read_text(encoding='utf-8') + '<EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745cc04",
   "metadata": {},
   "source": [
    "# Tokenize the Combined Text Corpus\n",
    "\n",
    "This cell reads the entire combined text file (`all_books.txt`) and tokenizes it using the `tiktoken` tokenizer. Tokenization is a crucial preprocessing step for training language models, as it converts raw text into a sequence of tokens (integers) that the model can understand.\n",
    "\n",
    "## What the Code Does\n",
    "- **Reads the full corpus**: Loads all text from `all_books.txt` into memory.\n",
    "- **Tokenizes the text**: Uses the `tokenizer.encode()` method to convert the text into a list of tokens.\n",
    "- **Prints the token count**: Displays the total number of tokens, which is useful for estimating dataset size and batching during training.\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip:** Knowing the total number of tokens helps you plan batch sizes, training steps, and memory requirements for your LLM training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792a586b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4962540\n"
     ]
    }
   ],
   "source": [
    "with(open('all_books.txt', 'r', encoding = 'utf-8') as textfile):\n",
    "    all_text = textfile.read()\n",
    "\n",
    "encoded_text = tokenizer.encode(all_text)\n",
    "\n",
    "print(f'Total number of tokens: {len(encoded_text)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c898e0",
   "metadata": {},
   "source": [
    "# Creating a PyTorch Dataset for Language Model Training\n",
    "\n",
    "This cell defines a custom `BooksDataset` class, which prepares tokenized text data for training an autoregressive language model (such as GPT). The dataset generates input and target sequences for each training example, enabling the model to learn to predict the next token in a sequence.\n",
    "\n",
    "## What the Code Does\n",
    "- **Initializes the Dataset**: Takes the full text, a tokenizer, a maximum sequence length (`max_length`), and a step size (step size between sequences).\n",
    "- **Tokenizes the Text**: Encodes the entire text into tokens using the provided tokenizer.\n",
    "- **Creates Overlapping Chunks**: For each position in the tokenized text, creates an input chunk of length `max_length` and a target chunk that is shifted by one token (the next-token prediction target).\n",
    "- **Stores as Tensors**: Converts each chunk into a PyTorch tensor for efficient batching and training.\n",
    "- **Implements `__len__` and `__getitem__`**: Standard PyTorch Dataset methods for compatibility with DataLoader.\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip:**\n",
    "> - Adjust `max_length` and `stride` to control the size and overlap of training samples.\n",
    "> - This dataset structure is ideal for next-token prediction tasks, where the model learns to generate text one token at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5aad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooksDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, step_size):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        encoded_text = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(encoded_text) - max_length, step_size):\n",
    "            input_chunk = encoded_text[i:i + max_length]\n",
    "            target_chunk = encoded_text[i + 1 : i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bebffe",
   "metadata": {},
   "source": [
    "# Utility Function: Create DataLoader for LLM Training\n",
    "\n",
    "This cell defines a helper function, `create_dataloader`, which streamlines the process of preparing a PyTorch DataLoader for language model training. It takes raw text and returns a DataLoader that yields batches of input and target token sequences, ready for model training.\n",
    "\n",
    "## What the Code Does\n",
    "- **Defines `create_dataloader`**: Accepts the full text, maximum sequence length, step size (stride), and batch size as arguments.\n",
    "- **Initializes the Tokenizer**: Uses the `cl100k_base` encoding from `tiktoken` for tokenization.\n",
    "- **Creates a `BooksDataset`**: Uses the custom dataset class to generate input-target pairs from the tokenized text.\n",
    "- **Builds a DataLoader**: Wraps the dataset in a PyTorch DataLoader for efficient batching, shuffling, and parallel data loading.\n",
    "- **Returns the DataLoader**: Ready to be used in a training loop for next-token prediction tasks.\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip:**\n",
    "> - Adjust `max_length`, `step_size`, and `batch_size` to fit your model and hardware constraints.\n",
    "> - This function abstracts away the repetitive setup code, making your training pipeline cleaner and more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36d302c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(text, max_length = 512, step_size = 256, batch_size = 8, shuffle = True):\n",
    "    tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "    dataset = BooksDataset(text,tokenizer, max_length, step_size)\n",
    "    dataloader = DataLoader(\n",
    "        dataset = dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = True,\n",
    "        num_workers = 0\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b365d47",
   "metadata": {},
   "source": [
    "# Example: Using the DataLoader for Batching\n",
    "\n",
    "This cell demonstrates how to use the `create_dataloader` utility to generate batches of input and target sequences for language model training. It shows how to iterate through the DataLoader and inspect the structure of each batch.\n",
    "\n",
    "## What the Code Does\n",
    "- **Creates a DataLoader**: Calls `create_dataloader` with the full text, a batch size of 2, a maximum sequence length of 8, and a step size of 4. Shuffling is disabled for demonstration purposes.\n",
    "- **Iterates Through Batches**: Loops through the DataLoader and prints the first 4 batches, showing the input and target tensors for each batch.\n",
    "- **Batch Structure**: Each batch contains a tuple of input and target tensors, where:\n",
    "  - The input tensor is a sequence of token IDs of length `max_length`.\n",
    "  - The target tensor is the same sequence shifted by one token (for next-token prediction).\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip:**\n",
    "> - Adjust `batch_size`, `max_length`, and `step_size` to match your model and hardware.\n",
    "> - Inspecting batches before training helps verify that your data pipeline is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d5b6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: [tensor([[ 3305,   791,  5907, 52686, 58610,   315,   578, 19121],\n",
      "        [58610,   315,   578, 19121, 21785,   315, 12656, 42482]]), tensor([[  791,  5907, 52686, 58610,   315,   578, 19121, 21785],\n",
      "        [  315,   578, 19121, 21785,   315, 12656, 42482,  7361]])]\n",
      "Batch 1: [tensor([[21785,   315, 12656, 42482,  7361,  2028, 35097,   374],\n",
      "        [ 7361,  2028, 35097,   374,   369,   279,  1005,   315]]), tensor([[  315, 12656, 42482,  7361,  2028, 35097,   374,   369],\n",
      "        [ 2028, 35097,   374,   369,   279,  1005,   315,  5606]])]\n",
      "Batch 2: [tensor([[  369,   279,  1005,   315,  5606, 12660,   304,   279],\n",
      "        [ 5606, 12660,   304,   279,  3723,  4273,   323,   198]]), tensor([[  279,  1005,   315,  5606, 12660,   304,   279,  3723],\n",
      "        [12660,   304,   279,  3723,  4273,   323,   198,  3646]])]\n",
      "Batch 3: [tensor([[3723, 4273,  323,  198, 3646, 1023, 5596,  315],\n",
      "        [3646, 1023, 5596,  315,  279, 1917,  520,  912]]), tensor([[4273,  323,  198, 3646, 1023, 5596,  315,  279],\n",
      "        [1023, 5596,  315,  279, 1917,  520,  912, 2853]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(all_text, batch_size = 2, max_length = 8, step_size = 4, shuffle = False)\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(f\"Batch {i}: {batch}\")\n",
    "    if i == 3:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc02fc",
   "metadata": {},
   "source": [
    "# Token and Positional Embeddings for LLMs\n",
    "\n",
    "This cell demonstrates how to combine token embeddings and positional embeddings, which are essential components in transformer-based language models (LLMs) like GPT. Token embeddings represent the meaning of each token, while positional embeddings encode the position of each token in the input sequence, allowing the model to capture word order and context.\n",
    "\n",
    "## What the Code Does\n",
    "- **Defines `vocab_size` and `embedding_dim`**: Sets the vocabulary size and embedding dimension for the model.\n",
    "- **Sets `context_length`**: Specifies the maximum sequence length (context window) the model can process.\n",
    "- **Creates Token Embedding Layer**: Maps each token ID to a dense vector of size `embedding_dim`.\n",
    "- **Creates Positional Embedding Layer**: Maps each position (from 0 to `context_length - 1`) to a dense vector of the same size.\n",
    "- **Loads a Batch of Data**: Uses the DataLoader to get a batch of input and target token sequences.\n",
    "- **Computes Token Embeddings**: Looks up embeddings for each token in the input batch.\n",
    "- **Computes Positional Embeddings**: Looks up embeddings for each position in the sequence.\n",
    "- **Combines Embeddings**: Adds token and positional embeddings to form the final input embeddings for the model.\n",
    "- **Prints Shapes**: Displays the shapes of the resulting tensors to verify correctness.\n",
    "\n",
    "---\n",
    "\n",
    "> **Tip:**\n",
    "> - Adding token and positional embeddings is a standard practice in transformer models, enabling them to understand both content and order of tokens.\n",
    "> - Ensure that `context_length` matches the maximum sequence length used during training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00e5d850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape: torch.Size([8, 1024, 512])\n",
      "Positional embeddings shape: torch.Size([1024, 512])\n",
      "Input embeddings shape: torch.Size([8, 1024, 512])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "embedding_dim = 512\n",
    "context_length = 1024\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "positional_embedding_layer = torch.nn.Embedding(context_length, embedding_dim)\n",
    "\n",
    "dataloader = create_dataloader(all_text, batch_size = 8, max_length = context_length, step_size = 512, shuffle = True)\n",
    "dataiter = iter(dataloader)\n",
    "\n",
    "test_input, test_targets = next(dataiter)\n",
    "\n",
    "\n",
    "token_embeddings = token_embedding_layer(test_input)\n",
    "print(f'Token embeddings shape: {token_embeddings.shape}')\n",
    "\n",
    "positional_embeddings = positional_embedding_layer(torch.arange(context_length))\n",
    "print(f'Positional embeddings shape: {positional_embeddings.shape}')\n",
    "\n",
    "input_embeddings = token_embeddings + positional_embeddings\n",
    "print(f'Input embeddings shape: {input_embeddings.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
